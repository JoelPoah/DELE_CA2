{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_addons in c:\\users\\p2112729\\.conda\\envs\\joel_gpu\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in c:\\users\\p2112729\\.conda\\envs\\joel_gpu\\lib\\site-packages (from tensorflow_addons) (2.13.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\p2112729\\.conda\\envs\\joel_gpu\\lib\\site-packages (from tensorflow_addons) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\p2112729\\.conda\\envs\\joel_gpu\\lib\\site-packages (from packaging->tensorflow_addons) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "# DL modules\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Sequential, Model\n",
    "import keras.backend as K\n",
    "from tensorflow.keras import initializers , layers\n",
    "from tensorflow_docs.vis import embed\n",
    "import imageio\n",
    "from tensorflow_addons.layers import SpectralNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# relevent libraries \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "## matplotlib stylings\n",
    "plt.rcParams['figure.figsize'] = 12, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_channels = 1\n",
    "num_classes = 10\n",
    "image_size = 28\n",
    "latent_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "# import dataset\n",
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "\n",
    "classes = {\n",
    "    0: 'Airplane',  1: 'Automobile',\n",
    "    2: 'Bird',      3: 'Cat',\n",
    "    4: 'Deer',      5: 'Dog',\n",
    "    6: 'Frog',      7: 'Horse',\n",
    "    8: 'Ship',      9: 'Truck' \n",
    "}\n",
    "\n",
    "y_train = y_train.reshape(-1)\n",
    "y_test  = y_test.reshape(-1)\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "y_train shape: (50000,)\n",
      "x_test shape: (10000, 32, 32, 3)\n",
      "y_test shape: (10000,)\n",
      "255\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print(x_train.max())\n",
    "print(x_train.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8), array([5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000],\n",
      "      dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_train, return_counts=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "## Pixel normalization\n",
    "> Dividing each of the pixels by 255 will normalize the pixels between -1 to 1 \n",
    "We normalize the pixels so that it can increase the speed of the learning process\n",
    "Neural Network processes inputs uses small weights values. Large inputs can disrupt or slow down learning process.\n",
    "It is good that we normalize the pixels. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 127.5 -1 \n",
    "x_test = x_test / 127.5 -1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIC OF GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = (32, 32, 3) # dimensions of training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self, rows, cols, channels, z = 128):\n",
    "        # Input shape\n",
    "        self.img_rows = rows\n",
    "        self.img_cols = cols\n",
    "        self.channels = channels\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = z\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        init = initializers.RandomNormal(stddev=0.02)\n",
    "\n",
    "        # Generator network\n",
    "        model = Sequential()\n",
    "\n",
    "        # FC: 2x2x512 \n",
    "        model.add(Dense(2*2*512, input_shape=(self.latent_dim,), kernel_initializer=init))\n",
    "        model.add(Reshape((2, 2, 512)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(0.2))\n",
    "\n",
    "        # # Conv 1: 4x4x256\n",
    "        model.add(Conv2DTranspose(256, kernel_size=5, strides=2, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(0.2))\n",
    "\n",
    "        # Conv 2: 8x8x128\n",
    "        model.add(Conv2DTranspose(128, kernel_size=5, strides=2, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(0.2))\n",
    "\n",
    "        # Conv 3: 16x16x64\n",
    "        model.add(Conv2DTranspose(64, kernel_size=5, strides=2, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(0.2))\n",
    "\n",
    "        # Conv 4: 32x32x3\n",
    "        model.add(Conv2DTranspose(3, kernel_size=5, strides=2, padding='same',\n",
    "                              activation='tanh'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=256, save_interval=50):\n",
    "\n",
    "\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of images\n",
    "            idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "            imgs = x_train[idx]\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"dcgan_mnist_{:d}.png\".format(epoch))\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 16, 16, 32)        896       \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 64)          18496     \n",
      "                                                                 \n",
      " zero_padding2d (ZeroPadding  (None, 9, 9, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 9, 9, 64)         256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 9, 9, 64)          0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 9, 9, 64)          0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 5, 5, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 5, 5, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 5, 5, 128)         0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5, 5, 128)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 5, 5, 256)         295168    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 5, 5, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 5, 5, 256)         0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 5, 5, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6400)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 6401      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 396,609\n",
      "Trainable params: 395,713\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 2048)              264192    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 2, 2, 512)        2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 4, 4, 256)        3277056   \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 4, 4, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 8, 8, 128)        819328    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 16, 16, 64)       204864    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 32, 32, 3)        4803      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,574,083\n",
      "Trainable params: 4,572,163\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "8/8 [==============================] - 7s 5ms/step\n",
      "0 [D loss: 1.120360, acc.: 30.27%] [G loss: 0.679447]\n",
      "1/1 [==============================] - 0s 300ms/step\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "1 [D loss: 1.265458, acc.: 47.66%] [G loss: 0.577095]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "2 [D loss: 1.132435, acc.: 48.83%] [G loss: 0.504945]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "3 [D loss: 0.854105, acc.: 54.88%] [G loss: 0.471672]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "4 [D loss: 0.753838, acc.: 55.27%] [G loss: 0.440031]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "5 [D loss: 0.543595, acc.: 70.70%] [G loss: 0.349287]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "6 [D loss: 0.442009, acc.: 81.05%] [G loss: 0.215929]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "7 [D loss: 0.323553, acc.: 90.04%] [G loss: 0.131491]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8 [D loss: 0.236193, acc.: 95.51%] [G loss: 0.061291]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "9 [D loss: 0.181135, acc.: 97.66%] [G loss: 0.047466]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "10 [D loss: 0.132921, acc.: 98.44%] [G loss: 0.018628]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "11 [D loss: 0.106087, acc.: 99.41%] [G loss: 0.020954]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "12 [D loss: 0.088683, acc.: 99.41%] [G loss: 0.010564]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "13 [D loss: 0.058888, acc.: 99.61%] [G loss: 0.011395]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "14 [D loss: 0.058251, acc.: 100.00%] [G loss: 0.002116]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "15 [D loss: 0.050347, acc.: 99.61%] [G loss: 0.003445]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "16 [D loss: 0.039467, acc.: 99.80%] [G loss: 0.003377]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "17 [D loss: 0.033169, acc.: 100.00%] [G loss: 0.002749]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "18 [D loss: 0.023979, acc.: 100.00%] [G loss: 0.003740]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "19 [D loss: 0.025956, acc.: 99.80%] [G loss: 0.007390]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "20 [D loss: 0.020462, acc.: 100.00%] [G loss: 0.002677]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "21 [D loss: 0.024081, acc.: 99.80%] [G loss: 0.003922]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "22 [D loss: 0.021309, acc.: 99.80%] [G loss: 0.002236]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "23 [D loss: 0.015632, acc.: 100.00%] [G loss: 0.001078]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "24 [D loss: 0.015444, acc.: 99.80%] [G loss: 0.001956]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "25 [D loss: 0.014157, acc.: 100.00%] [G loss: 0.003192]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "26 [D loss: 0.015627, acc.: 100.00%] [G loss: 0.002554]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "27 [D loss: 0.015314, acc.: 100.00%] [G loss: 0.001388]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "28 [D loss: 0.015505, acc.: 99.80%] [G loss: 0.001584]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "29 [D loss: 0.010017, acc.: 100.00%] [G loss: 0.002334]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "30 [D loss: 0.010797, acc.: 100.00%] [G loss: 0.001313]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "31 [D loss: 0.010206, acc.: 100.00%] [G loss: 0.002016]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "32 [D loss: 0.011813, acc.: 100.00%] [G loss: 0.000690]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "33 [D loss: 0.009948, acc.: 100.00%] [G loss: 0.003222]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "34 [D loss: 0.009558, acc.: 100.00%] [G loss: 0.001932]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "35 [D loss: 0.013115, acc.: 100.00%] [G loss: 0.001369]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "36 [D loss: 0.008931, acc.: 100.00%] [G loss: 0.001867]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "37 [D loss: 0.009841, acc.: 100.00%] [G loss: 0.001401]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "38 [D loss: 0.009172, acc.: 100.00%] [G loss: 0.001364]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "39 [D loss: 0.008878, acc.: 100.00%] [G loss: 0.001170]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "40 [D loss: 0.010054, acc.: 100.00%] [G loss: 0.002836]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "41 [D loss: 0.007646, acc.: 100.00%] [G loss: 0.001727]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "42 [D loss: 0.008827, acc.: 100.00%] [G loss: 0.001284]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "43 [D loss: 0.008115, acc.: 100.00%] [G loss: 0.001096]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "44 [D loss: 0.009494, acc.: 99.80%] [G loss: 0.001675]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "45 [D loss: 0.009550, acc.: 100.00%] [G loss: 0.000810]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "46 [D loss: 0.008791, acc.: 100.00%] [G loss: 0.001189]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "47 [D loss: 0.007307, acc.: 100.00%] [G loss: 0.001337]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "48 [D loss: 0.006050, acc.: 100.00%] [G loss: 0.001306]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "49 [D loss: 0.005633, acc.: 100.00%] [G loss: 0.001070]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "50 [D loss: 0.006465, acc.: 100.00%] [G loss: 0.001527]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "51 [D loss: 0.007367, acc.: 100.00%] [G loss: 0.001528]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "52 [D loss: 0.006764, acc.: 100.00%] [G loss: 0.001791]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "53 [D loss: 0.008160, acc.: 99.80%] [G loss: 0.001516]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "54 [D loss: 0.005879, acc.: 100.00%] [G loss: 0.000888]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "55 [D loss: 0.006907, acc.: 100.00%] [G loss: 0.001479]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "56 [D loss: 0.005331, acc.: 100.00%] [G loss: 0.002041]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "57 [D loss: 0.005979, acc.: 100.00%] [G loss: 0.000575]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "58 [D loss: 0.007220, acc.: 100.00%] [G loss: 0.001353]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "59 [D loss: 0.006444, acc.: 100.00%] [G loss: 0.001434]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "60 [D loss: 0.005417, acc.: 100.00%] [G loss: 0.001208]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "61 [D loss: 0.005900, acc.: 100.00%] [G loss: 0.001441]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "62 [D loss: 0.005687, acc.: 100.00%] [G loss: 0.001541]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "63 [D loss: 0.007063, acc.: 100.00%] [G loss: 0.000931]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "64 [D loss: 0.004416, acc.: 100.00%] [G loss: 0.001581]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "65 [D loss: 0.004712, acc.: 100.00%] [G loss: 0.003330]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "66 [D loss: 0.004542, acc.: 100.00%] [G loss: 0.001489]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "67 [D loss: 0.005270, acc.: 100.00%] [G loss: 0.001266]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "68 [D loss: 0.005906, acc.: 100.00%] [G loss: 0.000950]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "69 [D loss: 0.005528, acc.: 100.00%] [G loss: 0.001020]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "70 [D loss: 0.005130, acc.: 100.00%] [G loss: 0.001315]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "71 [D loss: 0.003880, acc.: 100.00%] [G loss: 0.000974]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "72 [D loss: 0.005504, acc.: 100.00%] [G loss: 0.001434]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "73 [D loss: 0.004704, acc.: 100.00%] [G loss: 0.001578]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "74 [D loss: 0.007987, acc.: 100.00%] [G loss: 0.001803]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "75 [D loss: 0.007096, acc.: 100.00%] [G loss: 0.001188]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "76 [D loss: 0.006946, acc.: 100.00%] [G loss: 0.001166]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "77 [D loss: 0.006307, acc.: 100.00%] [G loss: 0.001178]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "78 [D loss: 0.005543, acc.: 100.00%] [G loss: 0.001812]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "79 [D loss: 0.011687, acc.: 99.80%] [G loss: 0.001429]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "80 [D loss: 0.006410, acc.: 100.00%] [G loss: 0.001382]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "81 [D loss: 0.006565, acc.: 100.00%] [G loss: 0.002104]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "82 [D loss: 0.009154, acc.: 99.80%] [G loss: 0.002130]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "83 [D loss: 0.005957, acc.: 100.00%] [G loss: 0.001689]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "84 [D loss: 0.006023, acc.: 100.00%] [G loss: 0.001384]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "85 [D loss: 0.005876, acc.: 100.00%] [G loss: 0.001433]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "86 [D loss: 0.007406, acc.: 100.00%] [G loss: 0.001647]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "87 [D loss: 0.006253, acc.: 100.00%] [G loss: 0.003153]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "88 [D loss: 0.007598, acc.: 100.00%] [G loss: 0.002011]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "89 [D loss: 0.007429, acc.: 100.00%] [G loss: 0.001973]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "90 [D loss: 0.008885, acc.: 100.00%] [G loss: 0.002693]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "91 [D loss: 0.007870, acc.: 100.00%] [G loss: 0.001998]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "92 [D loss: 0.010725, acc.: 100.00%] [G loss: 0.001763]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "93 [D loss: 0.008429, acc.: 100.00%] [G loss: 0.002306]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "94 [D loss: 0.007750, acc.: 100.00%] [G loss: 0.004183]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "95 [D loss: 0.012595, acc.: 99.80%] [G loss: 0.003026]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "96 [D loss: 0.018362, acc.: 100.00%] [G loss: 0.002280]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "97 [D loss: 0.011122, acc.: 100.00%] [G loss: 0.005132]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "98 [D loss: 0.011205, acc.: 100.00%] [G loss: 0.005402]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "99 [D loss: 0.023735, acc.: 100.00%] [G loss: 0.009854]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "100 [D loss: 0.023407, acc.: 100.00%] [G loss: 0.016086]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "101 [D loss: 0.026908, acc.: 100.00%] [G loss: 0.015840]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "102 [D loss: 0.046411, acc.: 99.80%] [G loss: 0.163189]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "103 [D loss: 0.401268, acc.: 81.45%] [G loss: 13.892426]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "104 [D loss: 0.168538, acc.: 93.36%] [G loss: 18.113329]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "105 [D loss: 5.463881, acc.: 0.00%] [G loss: 9.134534]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "106 [D loss: 0.010457, acc.: 99.80%] [G loss: 6.893871]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "107 [D loss: 0.139734, acc.: 97.66%] [G loss: 1.839889]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "108 [D loss: 0.152137, acc.: 96.88%] [G loss: 0.944947]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "109 [D loss: 0.124430, acc.: 96.29%] [G loss: 0.396149]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "110 [D loss: 0.061046, acc.: 99.22%] [G loss: 0.196291]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "111 [D loss: 0.038307, acc.: 100.00%] [G loss: 0.156086]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "112 [D loss: 0.035722, acc.: 99.61%] [G loss: 0.125152]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "113 [D loss: 0.038861, acc.: 99.22%] [G loss: 0.132344]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "114 [D loss: 0.025687, acc.: 99.61%] [G loss: 0.144383]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "115 [D loss: 0.024845, acc.: 100.00%] [G loss: 0.130110]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "116 [D loss: 0.024310, acc.: 100.00%] [G loss: 0.137649]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "117 [D loss: 0.024060, acc.: 99.80%] [G loss: 0.121345]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "118 [D loss: 0.020830, acc.: 100.00%] [G loss: 0.103921]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "119 [D loss: 0.018499, acc.: 100.00%] [G loss: 0.102191]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "120 [D loss: 0.018818, acc.: 100.00%] [G loss: 0.086354]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "121 [D loss: 0.023076, acc.: 99.61%] [G loss: 0.079407]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "122 [D loss: 0.020316, acc.: 100.00%] [G loss: 0.088336]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "123 [D loss: 0.017898, acc.: 100.00%] [G loss: 0.074152]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "124 [D loss: 0.017540, acc.: 99.80%] [G loss: 0.084980]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "125 [D loss: 0.026463, acc.: 100.00%] [G loss: 0.076708]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "126 [D loss: 0.032365, acc.: 99.61%] [G loss: 0.091678]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "127 [D loss: 0.018754, acc.: 100.00%] [G loss: 0.092063]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "128 [D loss: 0.017446, acc.: 100.00%] [G loss: 0.088946]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "129 [D loss: 0.018632, acc.: 100.00%] [G loss: 0.100870]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "130 [D loss: 0.031379, acc.: 99.61%] [G loss: 0.078978]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "131 [D loss: 0.029481, acc.: 99.80%] [G loss: 0.127595]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "132 [D loss: 0.022121, acc.: 100.00%] [G loss: 0.160451]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "133 [D loss: 0.038853, acc.: 99.80%] [G loss: 0.137280]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "134 [D loss: 0.059537, acc.: 99.22%] [G loss: 0.224096]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "135 [D loss: 0.034692, acc.: 99.61%] [G loss: 0.321590]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "136 [D loss: 0.141208, acc.: 96.68%] [G loss: 1.460041]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "137 [D loss: 1.822133, acc.: 38.87%] [G loss: 11.566340]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "138 [D loss: 4.314851, acc.: 50.00%] [G loss: 3.295118]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "139 [D loss: 0.850878, acc.: 57.23%] [G loss: 5.269548]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "140 [D loss: 0.381581, acc.: 80.86%] [G loss: 3.559747]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "141 [D loss: 0.147887, acc.: 94.92%] [G loss: 0.402915]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "142 [D loss: 0.116431, acc.: 98.24%] [G loss: 0.546615]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "143 [D loss: 0.027746, acc.: 99.41%] [G loss: 0.862406]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "144 [D loss: 0.039694, acc.: 99.41%] [G loss: 0.472573]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "145 [D loss: 0.050858, acc.: 99.80%] [G loss: 0.334422]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "146 [D loss: 0.025104, acc.: 100.00%] [G loss: 0.359329]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "147 [D loss: 0.024734, acc.: 100.00%] [G loss: 0.388889]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "148 [D loss: 0.027810, acc.: 100.00%] [G loss: 0.356707]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "149 [D loss: 0.044061, acc.: 99.61%] [G loss: 0.341756]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "150 [D loss: 0.024270, acc.: 100.00%] [G loss: 0.297135]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "151 [D loss: 0.028390, acc.: 99.80%] [G loss: 0.269650]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "152 [D loss: 0.032261, acc.: 100.00%] [G loss: 0.266599]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "153 [D loss: 0.022980, acc.: 99.80%] [G loss: 0.282964]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "154 [D loss: 0.024246, acc.: 100.00%] [G loss: 0.255539]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "155 [D loss: 0.017456, acc.: 100.00%] [G loss: 0.252993]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "156 [D loss: 0.013900, acc.: 100.00%] [G loss: 0.249775]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "157 [D loss: 0.012875, acc.: 100.00%] [G loss: 0.259251]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "158 [D loss: 0.019631, acc.: 100.00%] [G loss: 0.248813]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "159 [D loss: 0.026427, acc.: 99.80%] [G loss: 0.247169]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "160 [D loss: 0.014356, acc.: 99.80%] [G loss: 0.231230]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "161 [D loss: 0.008704, acc.: 100.00%] [G loss: 0.195328]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "162 [D loss: 0.010053, acc.: 100.00%] [G loss: 0.187106]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "163 [D loss: 0.010113, acc.: 100.00%] [G loss: 0.183875]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "164 [D loss: 0.009156, acc.: 100.00%] [G loss: 0.173024]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "165 [D loss: 0.008555, acc.: 100.00%] [G loss: 0.203366]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "166 [D loss: 0.010545, acc.: 100.00%] [G loss: 0.196897]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "167 [D loss: 0.007874, acc.: 100.00%] [G loss: 0.163251]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "168 [D loss: 0.008612, acc.: 100.00%] [G loss: 0.171380]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "169 [D loss: 0.007524, acc.: 100.00%] [G loss: 0.164547]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "170 [D loss: 0.005914, acc.: 100.00%] [G loss: 0.167168]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "171 [D loss: 0.005078, acc.: 100.00%] [G loss: 0.180039]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "172 [D loss: 0.005110, acc.: 100.00%] [G loss: 0.171605]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "173 [D loss: 0.005174, acc.: 100.00%] [G loss: 0.161124]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "174 [D loss: 0.008607, acc.: 100.00%] [G loss: 0.194047]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "175 [D loss: 0.005974, acc.: 100.00%] [G loss: 0.133919]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "176 [D loss: 0.004582, acc.: 100.00%] [G loss: 0.147560]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "177 [D loss: 0.005372, acc.: 100.00%] [G loss: 0.127491]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "178 [D loss: 0.005432, acc.: 100.00%] [G loss: 0.148316]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "179 [D loss: 0.004074, acc.: 100.00%] [G loss: 0.177604]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "180 [D loss: 0.004260, acc.: 100.00%] [G loss: 0.157863]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "181 [D loss: 0.002989, acc.: 100.00%] [G loss: 0.171125]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "182 [D loss: 0.003598, acc.: 100.00%] [G loss: 0.187695]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "183 [D loss: 0.003576, acc.: 100.00%] [G loss: 0.173421]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "184 [D loss: 0.003023, acc.: 100.00%] [G loss: 0.162126]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "185 [D loss: 0.003912, acc.: 100.00%] [G loss: 0.169202]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "186 [D loss: 0.003115, acc.: 100.00%] [G loss: 0.158642]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "187 [D loss: 0.003399, acc.: 100.00%] [G loss: 0.143784]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "188 [D loss: 0.002798, acc.: 100.00%] [G loss: 0.150920]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "189 [D loss: 0.002544, acc.: 100.00%] [G loss: 0.164934]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "190 [D loss: 0.002856, acc.: 100.00%] [G loss: 0.163066]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "191 [D loss: 0.003578, acc.: 100.00%] [G loss: 0.112713]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "192 [D loss: 0.002632, acc.: 100.00%] [G loss: 0.160434]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "193 [D loss: 0.003247, acc.: 100.00%] [G loss: 0.180426]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "194 [D loss: 0.003212, acc.: 100.00%] [G loss: 0.158010]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "195 [D loss: 0.001776, acc.: 100.00%] [G loss: 0.117861]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "196 [D loss: 0.001672, acc.: 100.00%] [G loss: 0.117268]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "197 [D loss: 0.001724, acc.: 100.00%] [G loss: 0.130138]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "198 [D loss: 0.001819, acc.: 100.00%] [G loss: 0.124906]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "199 [D loss: 0.001902, acc.: 100.00%] [G loss: 0.140863]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "200 [D loss: 0.001656, acc.: 100.00%] [G loss: 0.116542]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "201 [D loss: 0.001652, acc.: 100.00%] [G loss: 0.099304]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "202 [D loss: 0.002130, acc.: 100.00%] [G loss: 0.099899]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "203 [D loss: 0.001194, acc.: 100.00%] [G loss: 0.089065]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "204 [D loss: 0.001438, acc.: 100.00%] [G loss: 0.109782]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "205 [D loss: 0.001673, acc.: 100.00%] [G loss: 0.113056]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "206 [D loss: 0.001123, acc.: 100.00%] [G loss: 0.106445]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "207 [D loss: 0.001300, acc.: 100.00%] [G loss: 0.128008]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "208 [D loss: 0.001325, acc.: 100.00%] [G loss: 0.138697]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "209 [D loss: 0.001052, acc.: 100.00%] [G loss: 0.091740]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "210 [D loss: 0.000876, acc.: 100.00%] [G loss: 0.086970]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "211 [D loss: 0.001025, acc.: 100.00%] [G loss: 0.126880]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "212 [D loss: 0.000898, acc.: 100.00%] [G loss: 0.115434]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "213 [D loss: 0.001003, acc.: 100.00%] [G loss: 0.103890]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "214 [D loss: 0.001013, acc.: 100.00%] [G loss: 0.106678]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "215 [D loss: 0.001098, acc.: 100.00%] [G loss: 0.114718]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "216 [D loss: 0.000744, acc.: 100.00%] [G loss: 0.117928]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "217 [D loss: 0.000619, acc.: 100.00%] [G loss: 0.097756]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "218 [D loss: 0.000957, acc.: 100.00%] [G loss: 0.117221]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "219 [D loss: 0.001183, acc.: 100.00%] [G loss: 0.132208]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "220 [D loss: 0.000513, acc.: 100.00%] [G loss: 0.083274]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "221 [D loss: 0.000929, acc.: 100.00%] [G loss: 0.115104]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "222 [D loss: 0.000893, acc.: 100.00%] [G loss: 0.093389]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "223 [D loss: 0.000667, acc.: 100.00%] [G loss: 0.088008]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "224 [D loss: 0.000846, acc.: 100.00%] [G loss: 0.103869]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "225 [D loss: 0.000739, acc.: 100.00%] [G loss: 0.099384]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "226 [D loss: 0.000580, acc.: 100.00%] [G loss: 0.098141]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "227 [D loss: 0.000932, acc.: 100.00%] [G loss: 0.100249]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "228 [D loss: 0.000693, acc.: 100.00%] [G loss: 0.111557]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "229 [D loss: 0.000627, acc.: 100.00%] [G loss: 0.093663]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "230 [D loss: 0.000814, acc.: 100.00%] [G loss: 0.088988]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "231 [D loss: 0.000697, acc.: 100.00%] [G loss: 0.122512]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "232 [D loss: 0.000429, acc.: 100.00%] [G loss: 0.080689]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "233 [D loss: 0.000854, acc.: 100.00%] [G loss: 0.108497]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "234 [D loss: 0.000576, acc.: 100.00%] [G loss: 0.106825]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "235 [D loss: 0.000580, acc.: 100.00%] [G loss: 0.078513]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "236 [D loss: 0.000503, acc.: 100.00%] [G loss: 0.074548]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "237 [D loss: 0.000864, acc.: 100.00%] [G loss: 0.107008]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "238 [D loss: 0.000506, acc.: 100.00%] [G loss: 0.119791]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "239 [D loss: 0.000457, acc.: 100.00%] [G loss: 0.089924]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "240 [D loss: 0.000605, acc.: 100.00%] [G loss: 0.086477]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "241 [D loss: 0.000346, acc.: 100.00%] [G loss: 0.091531]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "242 [D loss: 0.000476, acc.: 100.00%] [G loss: 0.094912]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "243 [D loss: 0.000656, acc.: 100.00%] [G loss: 0.073996]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "244 [D loss: 0.000409, acc.: 100.00%] [G loss: 0.100567]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "245 [D loss: 0.000768, acc.: 100.00%] [G loss: 0.073523]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "246 [D loss: 0.000445, acc.: 100.00%] [G loss: 0.085746]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "247 [D loss: 0.000659, acc.: 100.00%] [G loss: 0.080245]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "248 [D loss: 0.000521, acc.: 100.00%] [G loss: 0.088696]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "249 [D loss: 0.000461, acc.: 100.00%] [G loss: 0.078202]\n"
     ]
    }
   ],
   "source": [
    "dcgan = DCGAN(32,32,3,128)\n",
    "dcgan.train(epochs=250, batch_size=256, save_interval=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GG I DO NOT KNOW FROM HERE ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# story history object into dataframe\n",
    "hist_df = pd.DataFrame(hist.history)\n",
    "\n",
    "# plot the loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(hist_df['d_loss'], label='Discriminator Loss')\n",
    "plt.plot(hist_df['g_loss'], label='Generator Loss')\n",
    "plt.plot(hist_df['KL Divergence'], label='KL Divergence')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating 1000 Synthetic Images\n",
    "random_noise = tf.random.normal(shape=(1000, latent_dim))\n",
    "synthetic_images = gan.generator.predict(random_noise)\n",
    "print(\"Latent Vector Dim: {}\\tGenerated Images Dim: {}\".format(random_noise.shape, synthetic_images.shape))\n",
    "\n",
    "# Scaling back to [0, 1]\n",
    "synthetic_images -= -1\n",
    "synthetic_images /= (1 - (-1))\n",
    "\n",
    "# Display 15 randomly sampled images\n",
    "fig = plt.figure(figsize=(10, 10), tight_layout=True)\n",
    "for i in range(25):\n",
    "    rand_idx = np.random.randint(0, len(synthetic_images))\n",
    "    ax = fig.add_subplot(5, 5, i+1)\n",
    "    ax.imshow(synthetic_images[rand_idx])\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics to see accuracy \n",
    "## FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries to calculate FID\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.image import resize\n",
    "from scipy.linalg import sqrtm\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN_FID:\n",
    "    def __init__(self, batch_size, latent_dim, sample_size, buffer_size):\n",
    "        # setting Hyperparameters\n",
    "        self.BATCH_SIZE = batch_size\n",
    "        self.LATENT_DIM = latent_dim\n",
    "        self.SAMPLE_SIZE = sample_size\n",
    "        self.BUFFER_SIZE = buffer_size\n",
    "\n",
    "        # setting Constants\n",
    "        self.INCEPTION_SHAPE = (299, 299, 3)\n",
    "        self.INCEPTION = InceptionV3(include_top=False, pooling='avg', input_shape=self.INCEPTION_SHAPE)\n",
    "        self.AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "    # method to set generator and training data\n",
    "    def fit(self, generator, train_data):\n",
    "        # setting generative model and original data used for training \n",
    "        self.GENERATOR = generator\n",
    "        self.train_data = train_data\n",
    "\n",
    "        # Preparing Real Images\n",
    "        trainloader = tf.data.Dataset.from_tensor_slices((self.train_data))\n",
    "        trainloader = (\n",
    "            trainloader\n",
    "            .shuffle(self.BUFFER_SIZE)\n",
    "            .map(self.__resize_and_preprocess, num_parallel_calls=self.AUTO)\n",
    "            .batch(self.BATCH_SIZE, num_parallel_calls=self.AUTO)\n",
    "            .prefetch(self.AUTO)\n",
    "        )\n",
    "        self.trainloader = trainloader\n",
    "\n",
    "        # Generate and prepare Synthetic Images (Fake)\n",
    "        noise = tf.random.normal([self.SAMPLE_SIZE, self.LATENT_DIM])\n",
    "        generated_images = self.GENERATOR(noise)\n",
    "        genloader = tf.data.Dataset.from_tensor_slices(generated_images)\n",
    "        genloader = (\n",
    "            genloader\n",
    "            .map(self.__resize_and_preprocess, num_parallel_calls=self.AUTO)\n",
    "            .batch(self.BATCH_SIZE, num_parallel_calls=self.AUTO)\n",
    "            .prefetch(self.AUTO)\n",
    "        )\n",
    "        self.genloader = genloader\n",
    "\n",
    "        # prepare embeddings\n",
    "        count = math.ceil(self.SAMPLE_SIZE/self.BATCH_SIZE)\n",
    "\n",
    "        ## compute embeddings for real images\n",
    "        print(\"Computing Real Image Embeddings\")\n",
    "        self.real_image_embeddings = self.__compute_embeddings(self.trainloader, count)\n",
    "\n",
    "        ## compute embeddings for generated images\n",
    "        print(\"Computing Generated Image Embeddings\")\n",
    "        self.generated_image_embeddings = self.__compute_embeddings(self.genloader, count)\n",
    "        assert self.real_image_embeddings.shape == self.generated_image_embeddings.shape, \"Embeddings are not of the same size\"\n",
    "        print(\"Computed Embeddings\\tReal Images Embedding Shape: {}\\tGenerated Images Embedding Shape: {}\".format(\n",
    "            self.real_image_embeddings.shape, \n",
    "            self.generated_image_embeddings.shape\n",
    "        ))\n",
    "    \n",
    "    # method to produce evaluation results\n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def evaluate(self):\n",
    "        # calculate Frechet Inception Distance\n",
    "        fid = self.__calculate_fid(self.real_image_embeddings, self.generated_image_embeddings)\n",
    "        print('The computed FID score is:', fid)\n",
    "\n",
    "        return fid\n",
    "\n",
    "    # method to generate embeddings from inception model \n",
    "    def __compute_embeddings(self, dataloader, count):\n",
    "        image_embeddings = []\n",
    "        for _ in tqdm(range(count)):\n",
    "            images = next(iter(dataloader))\n",
    "            embeddings = self.INCEPTION.predict(images)\n",
    "            image_embeddings.extend(embeddings)\n",
    "        return np.array(image_embeddings)\n",
    "\n",
    "    ## STATIC METHODS: these methods knows nothing about the class\n",
    "    # static method to prepare the data before computing Inception Embeddings\n",
    "    @staticmethod\n",
    "    def __resize_and_preprocess(image):\n",
    "        # image *= 255.0 # original image are scaled to [0, 1], scaling back to [0, 255]\n",
    "        image -= -1\n",
    "        image /= (1 - (-1))\n",
    "        image *= 255.\n",
    "\n",
    "        # .preprocess_input() expects an image of scale [0, 255]\n",
    "        image = preprocess_input(image)\n",
    "        # inception model expects an image of shape (None, 299, 299, 3)\n",
    "        image = tf.image.resize(image, (299, 299), method='nearest')\n",
    "        return image\n",
    "\n",
    "    # static method to calculate frechet inception distance based on embeddings\n",
    "    @staticmethod \n",
    "    def __calculate_fid(real_embeddings, generated_embeddings):\n",
    "        # calculate mean and covariance statistics\n",
    "        mu1, sigma1 = real_embeddings.mean(axis=0), np.cov(real_embeddings, rowvar=False)\n",
    "        mu2, sigma2 = generated_embeddings.mean(axis=0), np.cov(generated_embeddings, rowvar=False)\n",
    "        # calculate sum squared difference between means\n",
    "        ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "        # calculate sqrt of product between cov\n",
    "        covmean = sqrtm(sigma1.dot(sigma2))\n",
    "        # check and correct imaginary numbers from sqrt\n",
    "        if np.iscomplexobj(covmean):\n",
    "            covmean = covmean.real\n",
    "        # calculate score\n",
    "        fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "        return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fid_class = GAN_FID(batch_size=512, latent_dim=128, sample_size=10000, buffer_size=1024)\n",
    "fid_class.fit(generator=gan.generator, train_data=x_train)\n",
    "fid_score = fid_class.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary information\n",
    "gan.generator.summary()\n",
    "\n",
    "# saving as .hdf format\n",
    "gan.generator.save(\"GAN_Generator.h5\")\n",
    "assert os.path.exists('GAN_Generator.h5'), \"GAN Generator does not exists\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('joel_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f87b2ba0695567226fd47786739bc0cb69a9f4337aa120baf744c6dd0597b59a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
